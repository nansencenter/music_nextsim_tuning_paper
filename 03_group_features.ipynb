{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pysida.lib import DAY_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_lims = np.array([\n",
    "    [0, 5],\n",
    "    [5, 10],\n",
    "    [10, 15],\n",
    "    [15, 20],\n",
    "    [20, 25],\n",
    "])\n",
    "\n",
    "edges_vec = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "d_vec = [1,2,4,8]\n",
    "\n",
    "propnames = [\n",
    "    'dissimilarity',\n",
    "    'homogeneity',\n",
    "    'ASM',\n",
    "    'energy',\n",
    "    'correlation',\n",
    "    'contrast',\n",
    "]\n",
    "\n",
    "\n",
    "def get_input_files(pfile):\n",
    "    print(pfile)\n",
    "    deforfile = pfile.replace('_pairs.npz', '_defor.npz')\n",
    "    anisofile = pfile.replace('_pairs.npz', '_aniso.npz')\n",
    "    propfile = pfile.replace('_pairs.npz', '_texture.npz')\n",
    "    scalingfile = pfile.replace('_pairs.npz', '_scale.npz')\n",
    "    lkfsfile = pfile.replace('_pairs.npz', '_lkf_stats.npz')\n",
    "\n",
    "    input_files = [\n",
    "        pfile,\n",
    "        deforfile,\n",
    "        anisofile,\n",
    "        propfile,\n",
    "        scalingfile,\n",
    "        lkfsfile,\n",
    "    ]\n",
    "    for input_file in input_files:\n",
    "        if not os.path.exists(input_file):\n",
    "            print('Where is ', input_file)\n",
    "            raise ValueError\n",
    "    return input_files\n",
    "\n",
    "def load_data(pfile, skip):\n",
    "    if pfile.split('/')[-1].split('_')[0] in skip:\n",
    "        print('Skip ', pfile)\n",
    "        raise ValueError\n",
    "    pfile, deforfile, anisofile, propfile, scalingfile, lkfsfile = get_input_files(pfile)\n",
    "    with np.load(pfile, allow_pickle=True) as f:\n",
    "        pairs = f['pairs']\n",
    "    with np.load(deforfile, allow_pickle=True) as f:\n",
    "        defor = f['defor']\n",
    "    with np.load(anisofile, allow_pickle=True) as f:\n",
    "        aniso = f['aniso']\n",
    "    with np.load(propfile, allow_pickle=True) as f:\n",
    "        props = f['props']\n",
    "    with np.load(scalingfile, allow_pickle=True) as f:\n",
    "        momes = f['mmm']\n",
    "        dates = list(f['dates'])\n",
    "    with np.load(lkfsfile, allow_pickle=True) as f:\n",
    "        lkfs = f['lkf_stats'].item()\n",
    "        lkfs = pd.DataFrame(lkfs, index=lkfs['dates'])\n",
    "    return pairs, defor, aniso, props, momes, lkfs, dates\n",
    "\n",
    "def get_defor_stats(pairs, defor, pair_indeces):\n",
    "    e1d_all = []\n",
    "    e1c_all = []\n",
    "    e2_all = []\n",
    "    for i in pair_indeces:\n",
    "        p = pairs[i]\n",
    "        d = defor[i]\n",
    "        e1 = d.e1[p.g] * DAY_SECONDS\n",
    "        e2 = d.e2[p.g] * DAY_SECONDS\n",
    "        e1d_all.append(np.log10(e1[e1 > 0]))\n",
    "        e1c_all.append(np.log10(-e1[e1 < 0]))\n",
    "        e2_all.append(np.log10(e2[e2 > 0]))\n",
    "    defor_stats = np.hstack([\n",
    "        np.percentile(np.hstack(e1d_all), [50, 90]),\n",
    "        np.percentile(np.hstack(e1c_all), [50, 90]),\n",
    "        np.percentile(np.hstack(e2_all),  [50, 90]),\n",
    "    ])\n",
    "    return defor_stats\n",
    "\n",
    "def get_aniso_stats(pairs, defor, aniso, pair_indeces):\n",
    "    ani_all = []\n",
    "    siz_all = []\n",
    "    edg_all = []\n",
    "\n",
    "    for i in pair_indeces:\n",
    "        p = pairs[i]\n",
    "        d = defor[i]\n",
    "        a = aniso[i]\n",
    "        if a is None or len(a) == 0:\n",
    "            continue\n",
    "        for cnt, edges in enumerate(edges_vec):\n",
    "            if f'ani|{edges}' in a:\n",
    "                gpi = np.isfinite(a[f'ani|{edges}']) * (a[f'ani|{edges}'] < 1)\n",
    "                ani_all.append(a[f'ani|{edges}'][gpi])\n",
    "                siz_all.append(a[f'siz|{edges}'][gpi])\n",
    "                edg_all.append(np.ones(gpi[gpi].size, float) * edges)\n",
    "\n",
    "    ani_all = np.hstack(ani_all)\n",
    "    siz_all = ((np.hstack(siz_all)/2)**0.5)/1000\n",
    "    edg_all = np.hstack(edg_all)\n",
    "    gpi = np.isfinite(ani_all) * (ani_all < 1) * (siz_all > (2*edg_all - 1))\n",
    "    ani_all = ani_all[gpi]\n",
    "    siz_all = siz_all[gpi]\n",
    "\n",
    "    ani_p50 = []\n",
    "    ani_p90 = []\n",
    "    for size_lim in size_lims:\n",
    "        gpi = (siz_all >= size_lim[0]) * (siz_all < size_lim[1])\n",
    "        if gpi[gpi].size == 0:\n",
    "            ani_p50.append(np.nan)\n",
    "            ani_p90.append(np.nan)\n",
    "        else:\n",
    "            ani_p50.append(np.median(ani_all[gpi]))\n",
    "            ani_p90.append(np.percentile(ani_all[gpi],90))\n",
    "    aniso_stats = np.hstack([ani_p50, ani_p90])\n",
    "    return aniso_stats\n",
    "\n",
    "def get_texture_stats(props, pair_indeces):\n",
    "    return props[pair_indeces].mean(axis=0).flatten()\n",
    "\n",
    "def get_lkf_stats(lkfs, dst_date):\n",
    "    return lkfs.loc[dst_date]['angles'], lkfs.loc[dst_date]['lengths'], lkfs.loc[dst_date]['counts']\n",
    "\n",
    "def read_features(pairs, defor, aniso, props, momes, lkfs, dates):\n",
    "    # dates of all pairs\n",
    "    pair_dates = [datetime(p.d0.year, p.d0.month, p.d0.day) if p else None for p in pairs]\n",
    "    # indices of MOM dates in the dates of all pairs\n",
    "    date_indeces = np.array([dates.index(pd) if pd in dates else -1 for pd in pair_dates])\n",
    "    # unique indices of MOM dates\n",
    "    date_indeces_unq = np.unique(date_indeces)\n",
    "    date_indeces_unq = date_indeces_unq[date_indeces_unq != -1]\n",
    "    # create date index for sampling LKFs\n",
    "    lkfs_i = lkfs.reindex(dates).interpolate(method='linear')\n",
    "\n",
    "    features = {}\n",
    "    for date_index in date_indeces_unq:\n",
    "        if momes[date_index] is None:\n",
    "            continue\n",
    "        dst_date = dates[date_index]\n",
    "        pair_indeces = np.where(date_indeces == date_index)[0]\n",
    "        defor_stats = get_defor_stats(pairs, defor, pair_indeces)\n",
    "        aniso_stats = get_aniso_stats(pairs, defor, aniso, pair_indeces)\n",
    "        textu_stats = get_texture_stats(props, pair_indeces)\n",
    "        scale_stats = momes[date_index]['c'].flatten()\n",
    "        lkf_stats = get_lkf_stats(lkfs_i, dst_date)\n",
    "        features[dst_date] = np.hstack([defor_stats, aniso_stats, textu_stats, scale_stats, lkf_stats])\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_valid_features_dates(features):\n",
    "    dates = np.array(list(features.keys()))\n",
    "    features = np.vstack([features[f] for f in features])\n",
    "    gpi = np.where(np.isfinite(np.sum(features, axis=1)))[0]\n",
    "    valid_features = features[gpi]\n",
    "    valid_dates = list(dates[gpi])\n",
    "    return valid_features, valid_dates\n",
    "\n",
    "def get_param_vals(exp_name, param_names):\n",
    "    param_vals = {}\n",
    "    tru_cfg_files = sorted(glob.glob(f'run_experiment/{exp_name}/sa10free_mat*cfg'))\n",
    "    for tru_cfg_file in tru_cfg_files:\n",
    "        exp_num = tru_cfg_file.split('/')[-1].split('.')[0].split('_')[1].replace('mat','')\n",
    "        param_vals[exp_num] = {}\n",
    "        with open(tru_cfg_file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            for param_name in param_names:\n",
    "                if param_name == line.split('=')[0]:\n",
    "                    param_vals[exp_num][param_name] = float(line.strip().split('=')[1])\n",
    "    return param_vals\n",
    "\n",
    "def merge_features_labels(param_vals, features_n, dates_n):\n",
    "    training_features = []\n",
    "    training_labels = []\n",
    "\n",
    "    for exp_num in param_vals:\n",
    "        param_vec = [param_vals[exp_num][param_name] for param_name in param_names]\n",
    "        if exp_num in features_n:\n",
    "            feature_vecs = features_n[exp_num]\n",
    "            dates_vec = dates_n[exp_num]\n",
    "            training_features.append(np.hstack([feature_vecs, np.array(dates_vec)[None].T]))\n",
    "            training_labels.append([param_vec] * len(feature_vecs))\n",
    "\n",
    "    training_features = np.vstack(training_features)\n",
    "    training_labels = np.vstack(training_labels)\n",
    "    return training_features, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_01\n",
    "idir = '../../music_matrix/cfg01_m20'\n",
    "odir = '../../music_matrix/cfg01_m20'\n",
    "exp_name = 'tru_cfg_01'\n",
    "param_names = ['compression_factor', 'C_lab', 'nu0', 'tan_phi']\n",
    "skip = ['mat09']\n",
    "\n",
    "feat_col_names = (\n",
    "    ['div_50', 'div_90', 'cnv_50', 'cnv_90', 'she_50', 'she_90'] +\n",
    "    [f'a50_{sl[0]:02}' for sl in size_lims] +\n",
    "    [f'a90_{sl[0]:02}' for sl in size_lims] +\n",
    "    [f'{propname[:3]}_{d:02}' for propname in propnames for d in d_vec] +\n",
    "    ['mom_1o', 'mom_1s', 'mom_2o', 'mom_2s', 'mom_3o', 'mom_3s'] +\n",
    "    ['lkf_an', 'lkf_ln', 'lkf_no']\n",
    ")\n",
    "\n",
    "print(len(feat_col_names), feat_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL FEATURES FROM ALL MEMBERS\n",
    "pfiles = sorted(glob.glob(f'{idir}/*pairs.npz'))\n",
    "print(len(pfiles), pfiles[0], pfiles[-1])\n",
    "\n",
    "features_n = {}\n",
    "dates_n = {}\n",
    "for pfile in pfiles:\n",
    "    try:\n",
    "        pairs, defor, aniso, props, momes, lkfs, dates = load_data(pfile, skip)\n",
    "    except (ValueError, BadZipFile) as e:\n",
    "        print(e)\n",
    "        print(pfile, 'is not processed')\n",
    "    else:\n",
    "        features = read_features(pairs, defor, aniso, props, momes, lkfs, dates)\n",
    "        features, dates = get_valid_features_dates(features)\n",
    "        member_id = pfile.split('/')[-1].split('_')[0].replace('mat','')\n",
    "        features_n[member_id] = features\n",
    "        dates_n[member_id] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE and SAVE TRAINING DATA FROM NEXTSIM\n",
    "param_vals = get_param_vals(exp_name, param_names)\n",
    "training_features, training_labels = merge_features_labels(param_vals, features_n, dates_n)\n",
    "trn_f_df = pd.DataFrame(training_features, columns=feat_col_names + ['date'])\n",
    "trn_l_df = pd.DataFrame(training_labels, columns=param_names)\n",
    "\n",
    "pd.to_pickle(trn_f_df, f'{odir}/ftrs.pickle')\n",
    "pd.to_pickle(trn_l_df, f'{odir}/lbls.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ AND SAVE RGPS DATA\n",
    "pfile = '../../rgps/csv/w07_may_pairs.npz'\n",
    "skip = 'skip'\n",
    "pairs, defor, aniso, props, momes, lkfs, dates = load_data(pfile, skip)\n",
    "features = read_features(pairs, defor, aniso, props, momes, lkfs, dates)\n",
    "features, dates = get_valid_features_dates(features)\n",
    "rgps_df = pd.DataFrame(np.hstack([features, np.array(dates)[None].T]), columns=feat_col_names + ['date'])\n",
    "pd.to_pickle(rgps_df, f'{odir}/rgps.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ftrs = pd.read_pickle(f'{odir}/ftrs.pickle')#.drop(columns=['date']).astype(float)\n",
    "inp_lbls = pd.read_pickle(f'{odir}/lbls.pickle')#.astype(float)\n",
    "inp_rgps = pd.read_pickle(f'{odir}/rgps.pickle')#.drop(columns=['date']).astype(float)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
